\chapter{Web extraction / TODO}

\textit{Web extraction} (also Web scraping, Web harvesting) is the extraction of structured data from a webpage presented as an HTML, i.e. semi-structured text. The access to the webpage might be done directly through the HTTP protocol (for example from the terminal) or with the browser which will actually render the page (we discussed it in details in section \nameref{sec:browser}). In case we do it through the HTTP request, we will get the raw information the same as a browser gets before it does all complicated prepossessing steps and displaying. In the current thesis, we will use both approaches and take advantage of each of them. We will discuss it in section \nameref{sec:urlparse}.

Since we know the main concepts related to a webpage and its structure, we can talk now about the Web extraction (WE) problem. In this section, we will discuss popular techniques used in this area as wrappers and adaptive methods, and also metrics for quality evaluation of extraction.\\  

It is important to highlight that by saying that we \textit{extract} the information from a webpage, we assume the information is actually available and readable. Also, we assume that when we extract the information, the page is available, what means the server returns 200 status code in a header of Hypertext Transfer Protocol (HTTP) request together with the content of the page.\\


\section{Information Extraction / TODO}

Important : Web Extraction is a Information Extraction from the web page. => the same sub tasks; Information Extraction = segmentation + classification + association + clustering. We have done only 2 steps. slide 62 in presentation \\

segmentation + classification = Named Entity Recognition\\
Association = Relation Extraction p.67\\


% https://web.stanford.edu/class/cs124/lec/Information_Extraction_and_Named_Entity_Recognition.pdf

\textit{Named Entity Recognition (NER)} is the task of extracting entities, such as proper name (person, location and organization), time (date and time), and numerical values (currency and percentage), from the source text, and mapping them into predefined categories, such as person, organization, location name or “none-of-the-above” [7]. The two main components is \textit{to find} the entity and then \textit{classify} this entity. \\

[7 A. Borthwick. A Maximum Entropy Approach to Named Entity Recognition. Phd thesis, New York University, 1999.]\\
% [lectures source https://web.stanford.edu/class/cs124/lec/Information_Extraction_and_Named_Entity_Recognition.pdf]

There three main techniques for NER (and IE): 
\te
Hand-written regular expressions\\
Using classifiers
• Generative: Naïve Bayes
• Discriminative: Maxent models\\
Sequence models
• HMMs
• CMMs/MEMMs\\


\section{Web extraction methods}

% READ WIKI https://en.wikipedia.org/wiki/Web_scraping#Techniques\\

The data on the Internet is massive and heterogeneous. Every website is following its own design and structure, presenting the information in a different way for the specific audience of the website. World Wide Web Consortium (W3C) is an international standard organization tries to get all the players on the Web implement the same set of core principles and components. That is why all websites use relatively the same stack of technologies, and browsers seek to follow all changes and support major updates in popular web tools.\\

Despite the fact, web developers use the same tools, HTML pages of different websites most of the time look completely different. For that reason the task of WE is still difficult: the algorithm which extracts the structured data from a webpage must take this diversity into account. The simplest way how information can be extracted from a webpage is human extracting but we won't consider this technique in this work since we consider only automatic or semi-automatic techniques. 

\subsection{Web crawlers}

One of the popular methods is so-called \textit{web crawlers} (or web spiders, web scrappers) which are basically using XPath and CSS selectors for extracting the information. The algorithm is very simple: you look to an HTML code, find those blocks which you need to extract, compose correct XPath or CSS selectors for it and after that, you can extract the information on any webpage where such path in a DOM tree exists. If you have one website with a big number of static pages with identical structure and design -- this method is the most appropriate and fast. \\

In our thesis, we used Scrapy Python framework for identifying the semantic Metadata tags and crawling the text corresponding to each tag. See \nameref{img:scrapy}.

\begin{lstlisting}[language=Python, caption={Example of meta tag text scrapping with Scrapy Python framework}, label={img:scrapy}, captionpos=b]
from scrapy.selector import Selector
import urllib.request as request

def get_item_types(url):
    item_types = []
    page = request.urlopen(url)
    if page.code == 200:
        html_body = page.read()
        selector = Selector(text=html_body, type="html")
        elements = selector.xpath('.//*[@itemscope]')
        for item in elements:
            item_type_text = item.xpath('@itemtype').extract()
            item_types.append(item_type_text)
    return item_types 
\end{lstlisting}
The code is self-explainable, but let's briefly discuss the main steps. Firstly you open the webpage with urllib, that's where we send an HTTP request. If the status code is 200, meaning the positive answer from a server, we read the page. We create Scrapy Selector and in this case we use XPath locator. We are identifying all elements which have 'itemscope' attribute meaning we want to extract all elements which tagged with any semantic label. Then in a loop we collect all types of these semantic tag names, for example 'http://schema.org/Person', 'http://schema.org/Movie', 'http://schema.org/Event', 'http://schema.org/Place', etc. The name of the Metadata tag takes the form of URL for simplicity. 

\subsection{Wrappers / TODO}
% http://www.isi.edu/integration/courses/csci548_2010/slides/Wrapper_Generation.pdf

\subsection{Adaptive information extraction/ TODO}

Important to say that it is Named Entity Recognition + Classification 
\subsection{Other techniques/ TODO}

WIKI:\\
Text pattern matching[edit]
A simple yet powerful approach to extract information from web pages can be based on the UNIX grep command or regular expression-matching facilities of programming languages (for instance Perl or Python).



\section{Other related problems / TODO}

\subsection{Region extraction / TODO}
\subsection{Visual segmentation / TODO}

% http://ijriest.com/IJRIEST_V1_I1_04.pdf

\section{Recall and Precision / TODO}

Named entity extraction\\
Precision, Recall, and the
F measure;\\
p.17 of presentation
% https://web.stanford.edu/class/cs124/lec/Information_Extraction_and_Named_Entity_Recognition.pdf
[source http://www.cs.sjtu.edu.cn/~li-fang/IEchapter8.pdf]

\section{Web extraction as a service}

Since the task of automatic web extraction is sought after, there are many applications written for solving this task. There are both desktop and web, free and paid applications with different functionality for the extraction of specific entities. Let's consider those services which provide this functionality as an API.\\

Such applications usually offer their services on a paid basis (per number of requests) and allow to use it by any other application through HTTP requests. These APIs normally extract only specific list of structured information. So it might be an API only for reviews or for example, only for product description in the online store. Below you will find popular web services and their main features.\\

\noindent \textbf{Diffbot's Automatic APIs} automatically extracts the content from supported page types: articles, discussions, images, video and products. It combines a variety of comprehensive techniques such as computer vision and machine learning. It accurately extracts clean and structured data from a webpage in JSON format without and prepossessing steps and training. One particular thing about Diffbot is that it can parse text in different languages because of extracting visual features, not only text content. [https://www.diffbot.com/]\\

\noindent\textbf{ParseHub} parses the structured data after one example given by the user. It has an interface where the user needs to select several elements which she wants to parse, compose the rule using the visual builder and then run this scenario for the entire page. ParseHub automatically explores all elements which the user meant and parse it to JSON format. After that, this script is available as an API.    
[https://www.parsehub.com/]\\

\noindent\textbf{Apifier} is similar to Diffbot extracts structured information for specific topics as a product, social networks details, booking hotel websites and search engine. It presents the data in JSON format. 
[source: https://www.apifier.com/] \\

\noindent\textit{Import.io} TODO Read about it
[source https://www.import.io/]

\noindent \textbf{IBM Watson} has an AlchemyLanguage (part of Natural Language Understanding API) component which analyzes the data on a webpage, extracts specific entities such as people, places, and organizations with NLP methods and answer questions about these entities and its relations. IBM Watson is a huge project and includes a lot of different components related to natural language understanding, speech, and visual recognition.     
[source https://www.ibm.com/watson/developercloud/alchemy-language.html] \\

\noindent\textbf{Other services}. There are dozens of web services which offer article content extraction, especially the name and description. Also there several interesting services which extract entities from an unstructured plain text: Ambiverse Natural Language Understanding, Google Cloud Natural Language. Entities are identified by types such as person, location, organization, or product.\\
% [source https://www.ambiverse.com/]\\
% [source https://www.programmableweb.com/category/extraction/api]\\
% [source https://cloud.google.com/natural-language/docs/reference/rest/]\\

\section{Related works / TODO}
